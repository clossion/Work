{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.utils import gen_batches\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from typing import *\n",
    "from numpy.linalg import *\n",
    "\n",
    "train_image_file = './mnist/train-images-idx3-ubyte'\n",
    "train_label_file = './mnist/train-labels-idx1-ubyte'\n",
    "test_image_file = './mnist/t10k-images-idx3-ubyte'\n",
    "test_label_file = './mnist/t10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "def decode_image(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(-1, 784)\n",
    "        images = np.array(images, dtype = float)\n",
    "    return images\n",
    "\n",
    "def decode_label(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic, n = struct.unpack('>II',f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        labels = np.array(labels, dtype = float)\n",
    "    return labels\n",
    "\n",
    "def load_data():\n",
    "    train_X = decode_image(train_image_file)\n",
    "    train_Y = decode_label(train_label_file)\n",
    "    test_X = decode_image(test_image_file)\n",
    "    test_Y = decode_label(test_label_file)\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = load_data()\n",
    "\n",
    "num_train, num_feature = trainX.shape\n",
    "plt.figure(1, figsize=(20,10))\n",
    "for i in range(8):\n",
    "    idx = np.random.choice(range(num_train))\n",
    "    plt.subplot(int('24'+str(i+1)))\n",
    "    plt.imshow(trainX[idx,:].reshape((28,28)))\n",
    "    plt.title('label is %d'%trainY[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the input value to make it between 0 and 1.\n",
    "trainX, testX = trainX/255, testX/255\n",
    "\n",
    "# convert labels to one-hot vector.\n",
    "def to_onehot(y):\n",
    "    y = y.astype(int)\n",
    "    num_class = len(set(y))\n",
    "    Y = np.eye((num_class))\n",
    "    return Y[y]\n",
    "\n",
    "trainY = to_onehot(trainY)\n",
    "testY = to_onehot(testY)\n",
    "num_train, num_feature = trainX.shape\n",
    "num_test, _ = testX.shape\n",
    "_, num_class = trainY.shape\n",
    "print('number of features is %d'%num_feature)\n",
    "print('number of classes is %d'%num_class)\n",
    "print('number of training samples is %d'%num_train)\n",
    "print('number of testing samples is %d'%num_test)\n",
    "print('shape of training data is ' + str(trainX.shape))\n",
    "print('shape of training data label is ' + str(trainX.shape))\n",
    "print('shape of testing data is ' + str(testX.shape))\n",
    "print('shape of testing data label is ' + str(testX.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "\n",
    "class Activation(ABC):\n",
    "    '''\n",
    "    An abstract class that implements an activation function\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Value of the activation function when input is x.\n",
    "        Parameters:\n",
    "          x is an input to the activation function.\n",
    "        Returns: \n",
    "          Value of the activation function. The shape of the return is the same as that of x.\n",
    "        '''\n",
    "        return x\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Derivative of the activation function with input x.\n",
    "        Parameters:\n",
    "          x is the input to activation function\n",
    "        Returns: \n",
    "          Derivative of the activation function w.r.t x.\n",
    "        '''\n",
    "        return x\n",
    "\n",
    "class Identity(Activation):\n",
    "    '''\n",
    "    Identity activation function. Input and output are identical. \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "    \n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        n, m = x.shape\n",
    "        return np.ones((n, m))\n",
    "    \n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    '''\n",
    "    Sigmoid activation function y = 1/(1 + e^(x*k)), where k is the parameter of the sigmoid function \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Parameters:\n",
    "          there are no parameters.\n",
    "        '''\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Parameters:\n",
    "          x is a two dimensional numpy array.\n",
    "        Returns:\n",
    "          a two dimensioal array representing the element-wise sigmoid of x.\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        return \n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Parameters:\n",
    "          x is a two dimensional array.\n",
    "        Returns:\n",
    "          a two dimensional array whose shape is the same as that of x. The returned value is the elementwise \n",
    "          derivative of the sigmoid function w.r.t. x.\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return\n",
    "    \n",
    "class ReLU(Activation):\n",
    "    '''\n",
    "    Rectified linear unit activation function\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        The derivative of the ReLU function w.r.t. x. Set the derivative to 0 at x=0.\n",
    "        Parameters:\n",
    "          x is a two dimensional array.\n",
    "        Returns:\n",
    "          elementwise derivative of ReLU. The shape of the returned value is the same as that of x.\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        \n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    '''\n",
    "    softmax nonlinear function.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        There are no parameters in softmax function.\n",
    "        '''\n",
    "        super(Softmax, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Parameters:\n",
    "          x is the input to the softmax function. x is a two dimensional numpy array. Each row is the input to the softmax function\n",
    "        Returns:\n",
    "          output of the softmax function. The returned value is with the same shape as that of x.\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        \n",
    "        \n",
    "        return\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Parameters:\n",
    "          x is the input to the softmax function. x is a two dimensional numpy array.\n",
    "        Returns:\n",
    "          a two dimensional array representing the derivative of softmax function w.r.t. x.\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural network for multi-class classification, the last layer is usually a softmax activation function. The output of the softmax function together with true targets or labels are used to compute the loss function. If both the softmax function and the loss function are computed independently, there are at least one drawbacks:\n",
    "\n",
    "> The derivative of the softmax function w.r.t. the input is a matrix, which is not like the elementwise derivative in ReLU or sigmoid. A batch of such derivatives forms a three dimensional tensor, making the computation complicated.\n",
    "\n",
    "To mitigate such an issue, a common trick is to merge the computation of softmax function with the loss function. In other words, the inputs to the loss function is the input to the softmax other than its output. In deep learning community, often the input to the softmax is regarded as unnormalized probability and is called logits. Being called logits is not exactly correct in math but is widely used. Let $\\boldsymbol{z}$ be the logits, the output of the softmax function $\\hat{\\boldsymbol{y}}$ is defined as\n",
    "\n",
    "$\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}}, i = 1, \\cdots, k$\n",
    "\n",
    "the cross entropy loss is computed as follows:\n",
    "\n",
    "$L(\\boldsymbol{z}, \\boldsymbol{y}) = \\sum_{i=1}^k y_i\\log(\\hat{y}_i) = \\sum_{i=1}^k y_i\\log\\left(\\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}}\\right) = \\sum_{i=1}^k y_i z_i - \\sum_{i=1}^k y_i\\log(\\sum_{j=1}^ke^{z_j})= \\sum_{i=1}^k y_i z_i - \\log(\\sum_{j=1}^ke^{z_j})$\n",
    "\n",
    "In the above expression, there is a log-sum-exponential term, which often appears in research papers and text book. The reason people prefer the log-sum-exponential term is that it is easy to deal with overflow problem. For instance, there is\n",
    "\n",
    "$L(\\boldsymbol{z}, \\boldsymbol{y}) = \\sum_{i=1}^k y_i z_i - \\log(\\sum_{j=1}^ke^{z_j}) = \\sum_{i=1}^k y_i z_i - \\log(\\sum_{j=1}^ke^{\\beta}e^{-\\beta}e^{z_j})= \\sum_{i=1}^k y_i z_i -\\beta - \\log(\\sum_{j=1}^ke^{z_j-\\beta})$\n",
    "\n",
    "By letting $\\beta = \\max z_i$, exponential terms in the exponential expression are all negative, avoiding the overflow problem.\n",
    "\n",
    "The derivative of the above loss function is computed as follows:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z_i} = y_i - \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}$\n",
    ", where the expression $\\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}$ is exactly the $i$th output of the softmax function. Thus,\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\boldsymbol{z}} = \\boldsymbol{y} - \\text{softmax}(\\boldsymbol{z})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# LOSS FUNCTIONS\n",
    "##################################################################################################################\n",
    "\n",
    "class Loss(ABC):\n",
    "    '''\n",
    "    Abstract class for a loss function\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def value(self, yhat: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''\n",
    "        Value of the empirical loss function.\n",
    "        Parameters:\n",
    "          y_hat is the output of a neural network. The shape of y_hat is (n, k). Each row represents the one sample output.\n",
    "          y contains true labels with shape (n, k).\n",
    "        Returns:\n",
    "          value of the empirical loss function.\n",
    "        '''\n",
    "        return 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Derivative of the empirical loss function with respect to the predictions.\n",
    "        Parameters:\n",
    "          \n",
    "        Returns:\n",
    "          The derivative of the loss function w.r.t. y_hat. The returned value is a two dimensional array with \n",
    "          shape (n, k)\n",
    "        '''\n",
    "        return yhat\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    '''\n",
    "    Cross entropy loss function\n",
    "    '''\n",
    "\n",
    "    def value(self, yhat: np.ndarray, y: np.ndarray) -> float:\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return\n",
    "\n",
    "    def derivative(self, yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return\n",
    "\n",
    "class CEwithLogit(Loss):\n",
    "    '''\n",
    "    Cross entropy loss function with logits (input of softmax activation function) and true labels as inputs.\n",
    "    '''\n",
    "    def value(self, logits: np.ndarray, y: np.ndarray) -> float:\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return\n",
    "        \n",
    "\n",
    "    def derivative(self, logits: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# METRICS\n",
    "##################################################################################################################\n",
    "\n",
    "def accuracy(y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "    '''\n",
    "    Accuracy of predictions, given the true labels.\n",
    "    Parameters:\n",
    "      y_hat is the outputs of softmax function. y_hat is with the shape (n, k).\n",
    "      y is the true targets. y is with the shape (n, k).\n",
    "    Returns:\n",
    "      accuracy which is a float number.\n",
    "    '''\n",
    "    #### write your code below ####\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the python class for neural network.\n",
    "# Using this class, users can design a neural network with any number of layers within which there could be any number of neuros\n",
    "\n",
    "class NeuralNetwork():\n",
    "    '''\n",
    "    Fully connected neural network.\n",
    "    Attributes:\n",
    "      n_layers is the number of layers.\n",
    "      activation is a list of Activation objects corresponding to each layer's activation function.\n",
    "      loss is a Loss object corresponding to the loss function used to train the network.\n",
    "      learning_rate is the learning rate.\n",
    "      W is a list of weight matrix used in each layer.\n",
    "      b is a list of biases used in each layer.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, layer_size: List[int], activation: List[Activation], loss: Loss, learning_rate: float = 0.01) -> None:\n",
    "        '''\n",
    "        Initializes a NeuralNetwork object\n",
    "        '''\n",
    "        assert len(activation) == len(layer_size), \\\n",
    "        \"Number of sizes for layers provided does not equal the number of activation\"\n",
    "        self.layer_size = layer_size\n",
    "        self.num_layer = len(layer_size)\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        for i in range(self.num_layer-1):\n",
    "            W = np.random.randn(layer_size[i], layer_size[i+1]) #/ np.sqrt(layer_size[i])\n",
    "            b = np.random.randn(1, layer_size[i+1])\n",
    "            self.W.append(W)\n",
    "            self.b.append(b)\n",
    "        self.A = []\n",
    "        self.Z = []\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> (List[np.ndarray], List[np.ndarray]):\n",
    "        '''\n",
    "        Forward pass of the network on a dataset of n examples with m features. Except the first layer, each layer\n",
    "        computes linear transformation plus a bias followed by a nonlinear transformation.\n",
    "        Parameters:\n",
    "          X is the training data with shape (n, m).\n",
    "        Returns:\n",
    "          A is a list of numpy data, representing the output of each layer after the first layer. There are \n",
    "            self.num_layer numpy arrays in the list and each array is of shape (n, self.layer_size[i]).\n",
    "          Z is a list of numpy data, representing the input of each layer after the first layer. There are\n",
    "            self.num_layer numpy arrays in the list and each array is of shape (n, self.layer_size[i]).\n",
    "        '''\n",
    "        num_sample = X.shape[0]\n",
    "        A, Z = [], []\n",
    "        #### write your code below ####\n",
    "        \n",
    "        \n",
    "        self.A = A\n",
    "        self.Z = Z\n",
    "        return Z, A\n",
    "\n",
    "    def backward(self, dLdyhat) -> List[np.ndarray]:\n",
    "        '''\n",
    "        Backward pass of the network on a dataset of n examples with m features. The derivatives are computed from \n",
    "          the end of the network to the front.\n",
    "        Parameters:\n",
    "          dLdyhat is the derivative of the empirical loss w.r.t. yhat which is the output of the neural network.\n",
    "            dLdyhat is with shape (n, self.layer_size[-1])\n",
    "        Returns:\n",
    "          dZ is a list of numpy array. Each numpy array in dZ represents the derivative of the emipirical loss function\n",
    "            w.r.t. the input of that specific layer. There are self.n_layer arrays in the list and each array is of \n",
    "            shape (n, self.layer_size[i])\n",
    "        '''\n",
    "        dZ = []\n",
    "        #### write your code below ####\n",
    "        \n",
    "        self.dZ = dZ\n",
    "        return dZ\n",
    "\n",
    "    def update_weights(self) -> List[np.ndarray]:\n",
    "        '''\n",
    "        Having computed the delta values from the backward pass, update each weight with the sum over the training\n",
    "        examples of the gradient of the loss with respect to the weight.\n",
    "        Parameters:\n",
    "          there is no input parameters\n",
    "        Returns:\n",
    "          W is the newly updated weights (i.e. self.W)\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def one_epoch(self, X: np.ndarray,  Y: np.ndarray, batch_size: int, train: bool = True)-> (float, float):\n",
    "        '''\n",
    "        One epoch of either training or testing procedure.\n",
    "        Parameters:\n",
    "          X is the data input. X is a two dimensional numpy array.\n",
    "          Y is the data label. Y is a one dimensional numpy array.\n",
    "          batch_size is the number of samples in each batch.\n",
    "          train is a boolean value indicating training or testing procedure.\n",
    "        Returns:\n",
    "          loss_value is the average loss function value.\n",
    "          acc_value is the prediction accuracy. \n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        slices = list(gen_batches(n, batch_size))\n",
    "        num_batch = len(slices)\n",
    "        idx = list(range(n))\n",
    "        np.random.shuffle(idx)\n",
    "        loss_value, acc_value = 0, 0\n",
    "        for i, index in enumerate(slices):\n",
    "            index = idx[slices[i]]\n",
    "            x, y = X[index,:], Y[index]\n",
    "            Z, A = model.forward(x)   # Execute forward pass\n",
    "            yhat = A[-1]\n",
    "            if train:\n",
    "                dLdz = self.loss.derivative(z, y)         # Calculate derivative of the loss with respect to out\n",
    "                self.backward(dLdz)     # Execute the backward pass to compute the deltas\n",
    "                self.update_weights()  # Calculate the gradients and update the weights\n",
    "            loss_value += self.loss.value(yhat, y)*x.shape[0]\n",
    "            acc_value += accuracy(yhat, y)*x.shape[0]\n",
    "        loss_value = loss_value/n\n",
    "        acc_value = acc_value/n\n",
    "        return loss_value, acc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : NeuralNetwork, X: np.ndarray, Y: np.ndarray, batch_size: int, epoches: int) -> (List[np.ndarray], List[float]):\n",
    "    '''\n",
    "    trains the neural network.\n",
    "    Parameters:\n",
    "      model is a NeuralNetwork object.\n",
    "      X is the data input. X is a two dimensional numpy array.\n",
    "      Y is the data label. Y is a one dimensional numpy array.\n",
    "      batch_size is the number of samples in each batch.\n",
    "      epoches is an integer, representing the number of epoches.\n",
    "    Returns:\n",
    "      epoch_loss is a list of float numbers, representing loss function value in all epoches.\n",
    "      epoch_acc is a list of float numbers, representing the accuracies in all epoches.\n",
    "    '''\n",
    "    loss_value, acc = model.one_epoch(X, Y, batch_size, train = False)\n",
    "    epoch_loss, epoch_acc = [loss_value], [acc]\n",
    "    print('Initialization: ', 'loss %.4f  '%loss_value, 'accuracy %.2f'%acc)\n",
    "    for epoch in range(epoches):\n",
    "        if epoch%100 == 0 and epoch > 0: # decrease the learning rate\n",
    "            model.learning_rate = min(model.learning_rate/10, 1.0e-5)\n",
    "        loss_value, acc = model.one_epoch(X, Y, batch_size, train = True)\n",
    "        if epoch%1 == 0:\n",
    "            print(\"Epoch {}/{}: Loss={}, Accuracy={}\".format(epoch, epoches, loss_value, acc))\n",
    "        epoch_loss.append(loss_value)\n",
    "        epoch_acc.append(acc)\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training procedure\n",
    "num_sample, num_feature = trainX.shape\n",
    "epoches = 200\n",
    "batch_size = 512\n",
    "Loss = []\n",
    "Acc = []\n",
    "learning_rate = 1/num_sample*batch_size\n",
    "np.random.seed(2023)\n",
    "model = NeuralNetwork([784, 256, 64, 10], [Identity(), ReLU(), ReLU(), Softmax()], CEwithLogit(), learning_rate = learning_rate)\n",
    "epoch_loss, epoch_acc = train(model, trainX, trainY, batch_size, epoches)\n",
    "\n",
    "# testing procedure\n",
    "test_loss, test_acc = model.one_epoch(testX, testY, batch_size, train = False)\n",
    "z, yhat = model.forward(testX)\n",
    "yhat = np.argmax(yhat, axis = 1)\n",
    "y = np.argmax(testY, axis = 1)\n",
    "print(confusion_matrix(yhat, y))\n",
    "print(classification_report(yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
